politeness
==========

A computational approach to politeness with application to social factors

The reference papers used for this project are in the folder `references'. Weka, an open source software that provides a collection of machine learning algorithms for data mining tasks is used for generating results. The files for this software are in the folder `weka-3-6-10'. The ARFF ((Attribute-Relation File Format) files generated by the training data and required for testing with weka are also in this folder. The `politeness_corpus' folder contains the training data used for the project. We have data from two different domains - `Wikipedia' and `Stack Exchange'. The code source files are in the folder `code/politeness/src'. The results of the experiments are in the folder `code/politeness/results'. The positive and negative word lexicons used for training classifiers are in the folder `code/politeness/data'.

To begin with, I used the `Training.java' and `ReadCSV.java' in 'code/politeness/src' to create the ARFF files required by weka to run classification experiments. An ARFF file is an ASCII text file that describes a list of instances sharing a set of attributes. The java program reads requests in the input csv file, and stores the information for each request. We assume that request receiving top 25% of the scores are positive (polite) and the bottom 25% are negative (impolite). The training and 5-fold cross-validation is done as follows:

1) We sort the training requests by their politeness scores.
2) Get top 25% of requests, and label them as positive.
3) Get bottom 25% of requests, and label them as negative.
4) Divide the data into 80% for training and 20% for testing.
5) Run SVM classifier training procedure on training data.
6) Test the classifier on testing data.
7) Go back to Step 4 to repeat the procedure for different sets of training and testing data, and then take the average performance.

Again, my training data is from two different domains:
1) Wikipedia
2) Stack Exchange

I experiment on two different types of classifiers: 
1) Bag of Words classifier (BOW)
2) Linguistically Informed classifer (Ling.)

I run experiments of two types:
1) In-domain:
    I use 5-fold cross-validations for these experiments. The experiments are:
    a) Training on Wikipedia, Testing on Wikipedia
    b) Training on Stack-Exchange, Testing on Stack-Exchange

2) Cross-domain:
    a) Training on Wikipedia, Testing on Stack-Exchange
    b) Training on Stack-Exchange, Testing on Wikipedia

For each experiment type and classifier type, I have four sets of experiments:
1) Using String-to-word unsupervised filter with alphabetic tokenizer (pre_alpha)
2) Using String-to-word unsupervised filter with alphabetic tokenizer followed by attribute selection (pre_alpha_with_attribute_selection)
3) Using String-to-word unsupervised filter with word tokenizer (pre_word)
4) Using String-to-word unsupervised filter with word tokenizer followed by attribute selection (pre_word_with_attribute_selection)

In each experiment set, I collected experiment results on these classifiers:
1) Naive Bayes
2) Naive Bayes Multinomial
3) J48
4) Random Forest with:
    a) 10 trees
    b) 100 trees
5) IBk (Instance-based k), the K-nearest neighbours classifier with:
    a) K=1 and using Euclidean distance
    b) K=10 and using Euclidean distance
    c) K=1 and using Manhattan distance
    d) K=10 and using Manhattan distance
6) SMO (Support vector classifier)

I use the following settings with String-to-word unsupervised filter:

IDFTransform: True
TFTransform: True
attributeIndices: first-last
doNotOperateOnFirstClassBasis: False
invertSelection: False
lowerCaseTokens: False
minTermFrequency: 10
normalizeDocLength: No Normalization
outputWordCounts: True
periodicPruning: -1.0
stemmer: NullStemmer
stopwords: weka-3-6-10
useStoplist: False
wordsToKeep: 1000

For attribute selection, I use:
evaluator: InfoGainAttributeEval and
search: Ranker with threshold 0.0

For Linguistically Informed classifer (Ling.), I use the following features:
1) Gratitude
2) Deference
3) Greeting
4) Positive lexicon
5) Negative lexicon
6) Apologizing
7) Please
8) Please start
9) Indirect (btw)
10) Direct question
11) Direct start
12) Counterfactual modal (Could/Would)
13) Indicative modal (Can/Will)


I now plan to use the classifiers I constructed above to derive a politeness score for blog entries. To begin with, I've gathered some blog entries from the blog:
http://ww2today.com/

I then constructed an ARFF file using the content of these entries. I'm now working on constructing automated logistic regression outputs for these blog entries.
These classification results should show a politeness score against the blog date and blog titles for each blog entry. This could be seen as an indicator of the change in sentiment over the events between 1938 and 1945 related to World War II in media.
I'd keep looking for blogs and other applications where change in politeness across time is a valuable metric.
