\documentclass[conference]{IEEEtran}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx} % Required to insert images
\usepackage{caption}
\newcommand{\Break}{\State \textbf{break} }
\algblockdefx[class]{class}{endclass}[1][]{\textbf{class} #1}{\textbf{End class}}
\algtext*{endclass}
\algblockdefx[method]{method}{endmethod}[1][]{\textbf{method} #1}{\textbf{End method}}
\algtext*{endmethod}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Comparative analysis of classifiers identifying politeness markings and application in web-logs}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Shagun Jhaver}
\IEEEauthorblockA{Dept. of Computer Science\\
The University of Texas at Dallas \\
Richardson, Texas 75080\\
Email: sxj124330@utdallas.edu}
}




% make the title area
\maketitle
\thispagestyle{plain}
\pagestyle{plain}


\begin{abstract}
%\boldmath
Given two strings $X$ and $Y$ over a finite alphabet, the edit distance between $X$ and $Y$, $d(X, Y)$ is the number of elimentary edit operations required to edit $X$ into $Y$. A dynamic programming algorithm elegantly computes this distance. In this paper, we investigate the parallelization of calculating edit distance for a large set of strings using MapReduce, a popular parallel computing framework. We propose SIM\_MR and PRE\_MR algorithms, parallel versions of the dynamic programming solution, and present implementations of these algorithms

Index Terms - Politeness Theory, Classification, SVM
\end{abstract}
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle


\section{Introduction}
Politeness, deference and tact have a sociological significance altogether beyond the level of table manners and etiquette books (Goffman 1971:90). Politeness, introduced into linguistics more than forty years ago, has emerged as a vital and rapidly developing area of study. Brown and Levinson's (1978, 1987) classic treatment of linguistic politeness show that politeness strategies are a basis for social order. The concepts inherent to their model have been invoked in much subsequent literature which has focused on linguistic carriers of politeness (e.g., speech acts, syntactic constructions, lexical items, etc.), seeking to quantify them, to compare them across cultures and genders, and to identify universals \cite{Meier}.

Danescu-Niculescu-Mizil, Sudhof, Jurafsky, Leskovec and Potts \cite{Jurafsky} develop a computational framework for identifying and characterizing the linguistic aspects of politeness. Their investigation is guided by a new corpus of requests annotated for politeness, that they constructed and released. This corpus consists of a large collection of requests from two different sources - Wikipedia and Stack Exchange. Both of these are large online communities in which users frequently make requests of other members. 

In this paper, we use this richly labeled data for politeness to construct politeness classifiers using different supervised and unsupervised machine learning algorithms, and present a comparative analysis of the performance of these classifiers. We also study the improvement in classifiers' performance after they use a wide range of lexical, sentiment and dependency features operationalizing key components of politeness theory. 

We observe that some of our classifiers achieve near human-level accuracy across different test-sets, which demonstrates the consistent nature of politeness strategies, and we use these classifiers with new data for further analysis of the relation of politeness to social factors. We select the web-log (blog) entries from blogs focused at different interest groups, assign these entries a politeness score on a scale of 0 to 1 using our classifiers, and compare these scores. 


\section{Background}
The meaning of politeness and concomitant concepts, and the claims for universals have shown considerable divergence and lack of clarity as they have received increased attention since Brown and Lewinson's proposed framework \cite{Meier}, \cite{Brown1}, \cite{Brown2}. Scholars use a variety of approaches to an account of politeness: the social-norm view, the conversational-maxim view; the face-saving view; and the conversational-contract view \cite{Fraser}. While none of these views is considered adequate, the face-saving view by Brown and Levinson is seen as the most clearly articulated and is the most popular.

Brown and Levinson contend that linguistic politeness must be communicated, that it constitutes a message. They assert that the failure to communicate the intention to be polite may be taken as absence of the required polite attitude. They propose a framework to explain politeness in which their rational Model Person has `face', the individual's self-esteem. This face is a culturally elborated public self-image that every member of a society wants to claim for himself \cite{Fraser}. They characterize two types of face in terms of participant wants rather than social norms:

Negative Face:
``the want of every `competent adult member' that his action be unimpeded by others" 

Positive Face:
``the want of every member that his wants be desirable to at least some others"

The organizing principle for their politeness theory is the idea that ``some acts are intrinsically threatening to face and thus require softening ..." To this end, each group of language users develops politeness principles from which they derive certain linguistic strategies. It is by the use of these so-called politeness strategies that speakers succeed in communicating both their primary message(s) as well as their intention to be polite in doing so. And in doing so, they reduce the face loss that results from the interaction. 

The choice of a specific linguistic form is to be viewed as a specific realization of one of the politeness strategies in light of the speaker's assessment of the utterance context. Brown and Levinson outline four main types of politeness strategies: bald on-record, negative politeness, positive politeness, and off-record (indirect). The speaker must choose a linguistic means that will satisfy the strategic end. Since each strategy embraces a range of degrees of politeness, the speaker will be required to consider the specific linguistic forms used and their overall effect when used in conjunction with one another.

We try to identify such strategies and use them to construct our classifiers. A brief description of the classifiers we used is given below.

1) \textit{Naive Bayes}: Naive Bayes is a highly practical learning method whose performance is shown to be comparable to that of neural network and decision tree learning in some domains. It applies to the learning tasks where each instance $x$ is described by a conjunction of attribute values and where the target function $f(x)$ can take on any value from some finite set $V$. A set of training examples of the target function is provided, and a new instance is presented, described by the tuple of attribute values $\textless a_l, a_2,..., a_n\textgreater$. The learner is asked to predict the target value, or classification, for this new instance. The Bayesian approach to classifying the new instance is to assign the most probable target value, $V_{MAP}$, given the attribute values $\textless a_l, a_2,..., a_n\textgreater$ that describe the instance \cite{Mitchell}. 

The naive Bayes classifier is based on the simplifying assumption that the attribute values are conditionally independent given the target value. In other words, the assumption is that given the target value of the instance, the probability of observing the conjunction $a_l, a_2,..., a_n$ is just the product of the probabilities for the individual attributes.

2) \textit{Naive Bayes Multinomial}: In the multinomial model, a document is an ordered sequence of word events, drawn from the same vocabulary V. We assume that the lengths of documents are independent of class. We again make a similar naive Bayes assumption: that the probability of each word event in a document is independent of the wordâ€™s context and position in the document. Thus, each document $d_i$ is drawn from a multinomial distribution of words with as many independent trials as the length of $d_i$. This yields the familiar ``bag of words" representation for documents \cite{Nigam}. Whereas simple naive Bayes would model a document as the presence and absence of particular words, multinomial naive bayes explicitly models the word counts and adjusts the underlying calculations to deal with in. 

3) \textit{J48}: J48 is an open source Java implementation of the C4.5 algorithm in the weka data mining tool. C4.5 is an algorithm used to generate a decision tree developed by Ross Quinlan. C4.5 is an extension of Quinlan's earlier ID3 algorithm. The decision trees generated by C4.5 can be used for classification, and for this reason, C4.5 is often referred to as a statistical classifier. At each node of the tree, C4.5 chooses the attribute of the data that most effectively splits its set of samples into subsets enriched in one class or the other. The splitting criterion is the normalized information gain (difference in entropy). The attribute with the highest normalized information gain is chosen to make the decision. The C4.5 algorithm then recurses on the smaller sublists \cite{j48}.

4) \textit{Random Forest}: Random Forests grows many classification trees. To classify a new object from an input vector, put the input vector down each of the trees in the forest. Each tree gives a classification, and we say the tree "votes" for that class. The forest chooses the classification having the most votes (over all the trees in the forest).

Each tree is grown as follows:

\begin{enumerate}
\item If the number of cases in the training set is N, sample N cases at random - but with replacement, from the original data. This sample will be the training set for growing the tree.
\item If there are M input variables, a number m$<<$M is specified such that at each node, m variables are selected at random out of the M and the best split on these m is used to split the node. The value of m is held constant during the forest growing.
\item Each tree is grown to the largest extent possible. There is no pruning.
\end{enumerate}

In the original paper on random forests, it was shown that the forest error rate depends on two things:

\begin{enumerate}
\item The correlation between any two trees in the forest. Increasing the correlation increases the forest error rate.
\item The strength of each individual tree in the forest. A tree with a low error rate is a strong classifier. Increasing the strength of the individual trees decreases the forest error rate.
\end{enumerate}

Reducing m reduces both the correlation and the strength. Increasing it increases both. Somewhere in between is an ``optimal" range of m - usually quite wide. Using the oob error rate a value of m in the range can quickly be found. This is the only adjustable parameter to which random forests is somewhat sensitive \cite{breiman}.

5) \textit{IBk}: This is an implementation of the k-nearest neighbors algorithm. This basic instance-based algorithm assumes all instances correspond to points in the n-dimensional space. The nearest neighbors of an instance are defined in terms of the standard Euclidean or Manhattan distance. The value of the classification label for an input x returned by this algorithm is just the most common value of label among the k training examples nearest to x \cite{Mitchell}.

6) \textit{SMO}: Sequential minimal optimization (SMO) is an algorithm for solving the optimization problem which arises during the training of support vector machines. It was invented by John Platt in 1998 at Microsoft Research. SMO is widely used for training support vector machines and is implemented by the popular LIBSVM tool. SMO breaks the optimization problem in SVM into a series of smallest possible sub-problems, which are then solved analytically \cite{smo}.

\section{Proposed Approach}

\section{Experimental Results}

This section describes the results for the experiments.

\subsection{In-domain Experiments}
Four sets of experiments are done for in-domain analysis using a 5-fold cross-validation. 

The correctly classified instances (by \%) for In-domain analysis on Wikipedia requests using Bag of Words classifiers are shown in table \ref{table:in-domain-wiki-BOW}.

\begin{table*}[htbp]
\caption{In-domain analysis on Wikipedia requests using Bag of Words classifiers }
\centering
\vspace{5pt}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Classifier} & \textbf{pre\_alpha} & \textbf{pre\_word} & \textbf{pre\_alpha\_with\_attribute\_selection} & \textbf{pre\_word\_with\_attribute\_selection} \\
\hline\hline
\textbf{Naive Bayes} & 74.5864\% & 74.4945\% & 77.2518\% & 77.068\% \\ 
\hline
\textbf{Naive Bayes Multinomial} & 78.9063\% & 79.6415\% & 80.5147\% & 80.1471\% \\ 
\hline
\textbf{J48} & 70.864\% & 71.2776\% & 73.7132\% & 74.3107\% \\ 
\hline
\textbf{Random Forest (10 trees)} & 74.5404\% & 73.6673\% & 76.7004\% & 76.6085\% \\ 
\hline
\textbf{Random Forest (100 trees)} & 80.7445\% & 80.193\% & 80.3309\% & 79.8254\% \\ 
\hline
\textbf{iBK (k=1, using Euclidean Distance)} & 64.8897\% & 64.1544\% & 71.2316\% & 70.6342\% \\ 
\hline
\textbf{iBK (k=10, using Euclidean Distance)} & 59.1912\% & 58.9154\% & 76.7463\% & 76.7923\% \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 63.2813\% & 63.1434\% & 71.2316\% & 69.1636\% \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 56.4338\% & 56.1581\% & 74.6783\% & 73.4835\% \\ 
\hline
\textbf{SMO} & 80.193\% & 79.8713\% & 82.307\% & 82.2151\% \\ 
\hline
\hline
\end{tabular}
\label{table:in-domain-wiki-BOW}
\end{table*}

The correctly classified instances (by \%) for In-domain analysis on Wikipedia requests using Linguistic classifiers are shown in table \ref{table:in-domain-wiki-Ling}.

\begin{table*}[htbp]
\caption{In-domain analysis on Wikipedia requests using Linguistic classifiers }
\centering
\vspace{5pt}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Classifier} & \textbf{pre\_alpha} & \textbf{pre\_word} & \textbf{pre\_alpha\_with\_attribute\_selection} & \textbf{pre\_word\_with\_attribute\_selection} \\
\hline\hline
\textbf{Naive Bayes} & 74.4485\% & 74.7702\% & 77.0221\% & 76.3327\% \\ 
\hline
\textbf{Naive Bayes Multinomial} & 80.7904\% & 80.239\% & 80.4688\% & 80.3309\% \\ 
\hline
\textbf{J48} & 72.7022\% & 72.4265\% & 75\% & 73.3456\% \\ 
\hline
\textbf{Random Forest (10 trees)} & 72.932\% & 74.7702\% & 76.7004\% & 77.4357\% \\ 
\hline
\textbf{Random Forest (100 trees)} & 79.9173\% & 80.6066\% & 80.1011\% & 80.4228\% \\ 
\hline
\textbf{iBK (k=1, using Euclidean Distance)} & 64.6599\% & 64.8438\% & 71.4614\% & 71.829\% \\ 
\hline
\textbf{iBK (k=10, using Euclidean Distance)} & 60.2022\% & 59.6967\% & 76.5165\% & 76.7923\% \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 64.6599\% & 64.8897\% & 70.5423\% & 70.5423\% \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 59.4669\% & 59.5129\% & 74.6783\% & 74.9081\% \\ 
\hline
\textbf{SMO} & 81.3879\% & 80.3768\% & 82.2151\% & 81.0202\% \\ 
\hline
\hline
\end{tabular}
\label{table:in-domain-wiki-Ling}
\end{table*}

The correctly classified instances (by \%) for In-domain analysis on Stack Exchange requests using Bag of Words classifiers are shown in table \ref{table:in-domain-stack-BOW}.

\begin{table*}[htbp]
\caption{In-domain analysis on Stack Exchange requests using Bag of Words classifiers }
\centering
\vspace{5pt}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Classifier} & \textbf{pre\_alpha} & \textbf{pre\_word} & \textbf{pre\_alpha\_with\_attribute\_selection} & \textbf{pre\_word\_with\_attribute\_selection} \\
\hline\hline
\textbf{Naive Bayes} & 68.4\% & 67.7\% & 71.8\% & 71.2\% \\ 
\hline
\textbf{Naive Bayes Multinomial} & 71.8\% & 71.55\% & 72.35\% & 71.4\% \\ 
\hline
\textbf{J48} & 67.15\% & 65.9\% & 69.05\% & 69.75\% \\ 
\hline
\textbf{Random Forest (10 trees)} & 69.5\% & 68.75\% & 70.15\% & 69.95\% \\ 
\hline
\textbf{Random Forest (100 trees)} & 73.6\% & 73.45\% & 72.35\% & 72.45\% \\ 
\hline
\textbf{iBK (k=1, using Euclidean Distance)} & 57.7\% & 58.85\% & 65.75\% & 65.6\% \\ 
\hline
\textbf{iBK (k=10, using Euclidean Distance)} & 53.2\% & 53.15\% & 66.95\% & 66.35\% \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 56.9\% & 56.85\% & 65.9\% & 63.55\% \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 51.35\% & 51.95\% & 64.6\% & 63.1\% \\ 
\hline
\textbf{SMO} & 74.55\% & 73.5\% & 74.8\% & 75.05\% \\ 
\hline
\hline
\end{tabular}
\label{table:in-domain-stack-BOW}
\end{table*}

The correctly classified instances (by \%) for In-domain analysis on Stack Exchange requests using Linguistic classifiers are shown in table \ref{table:in-domain-stack-Ling}.

\begin{table*}[htbp]
\caption{In-domain analysis on Stack Exchange requests using Linguistic classifiers }
\centering
\vspace{5pt}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Classifier} & \textbf{pre\_alpha} & \textbf{pre\_word} & \textbf{pre\_alpha\_with\_attribute\_selection} & \textbf{pre\_word\_with\_attribute\_selection} \\
\hline\hline
\textbf{Naive Bayes} & 68.45\% & 68.55\% & 72.4\% & 72.4\% \\ 
\hline
\textbf{Naive Bayes Multinomial} & 72.85\% & 72.7\% & 74.6\% & 74.4\% \\ 
\hline
\textbf{J48} & 67.7\% & 67.25\% & 71.1\% & 69.65\% \\ 
\hline
\textbf{Random Forest (10 trees)} & 69.15\% & 67.35\% & 71.6\% & 70.25\% \\ 
\hline
\textbf{Random Forest (100 trees)} & 74.2\% & 74.15\% & 73.35\% & 72.75\% \\ 
\hline
\textbf{iBK (k=1, using Euclidean Distance)} & 58.4\% & 59.2\% & 64.9\% & 63.5\% \\ 
\hline
\textbf{iBK (k=10, using Euclidean Distance)} & 55.25\% & 57.65\% & 71.2\% & 71.15\% \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 58.3\% & 58.6\% & 63.8\% & 63.15\% \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 53.55\% & 54.45\% & 68.8\% & 67.85\% \\ 
\hline
\textbf{SMO} & 72.95\% & 73.95\% & 75\% & 75.95\% \\ 
\hline
\hline
\end{tabular}
\label{table:in-domain-stack-Ling}
\end{table*}

\subsection{Cross-domain Experiments}
Four sets of experiments are done for cross-domain analysis using a 5-fold cross-validation. 

The correctly classified instances (by \%) for Cross-domain analysis with Wikipedia requests for training and Stack Exchange requests for testing and using Bag of Words classifiers are shown in table \ref{table:cross-domain-wiki-BOW}.

\begin{table*}[htbp]
\caption{Cross-domain analysis with Wikipedia requests for training and Stack Exchange requests for testing and using Bag of Words classifiers }
\centering
\vspace{5pt}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Classifier} & \textbf{pre\_alpha} & \textbf{pre\_word} & \textbf{pre\_alpha\_with\_attribute\_selection} & \textbf{pre\_word\_with\_attribute\_selection} \\
\hline\hline
\textbf{Naive Bayes} & 63.1\% & 62.7\% & 65.35\% & 64.85\% \\ 
\hline
\textbf{Naive Bayes Multinomial} & 66.45\% & 66\% & 66.55\% & 66.5\% \\ 
\hline
\textbf{J48} & 62.55\% & 61.6\% & 60.4\% & 61.1\% \\ 
\hline
\textbf{Random Forest (10 trees)} & 64.85\% & 64.95\% & 64.05\% & 64.35\% \\ 
\hline
\textbf{Random Forest (100 trees)} & 66.2\% & 66.25\% & 64.65\% & 64.65\% \\ 
\hline
\textbf{iBK (k=1, using Euclidean Distance)} & 55.05\% & 55\% & 62.6\% & 63.25\% \\ 
\hline
\textbf{iBK (k=10, using Euclidean Distance)} & 50.45\% & 50.25\% & 61.15\% & 61.35\% \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 54.7\% & 54.3\% & 61.05\% & 60.55\% \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 50.5\% & 50.35\% & 58.35\% & 58.75\% \\ 
\hline
\textbf{SMO} & 64.65\% & 65.55\% & 65.35\% & 64.4\% \\ 
\hline
\hline
\end{tabular}
\label{table:cross-domain-wiki-BOW}
\end{table*}

The correctly classified instances (by \%) for Cross-domain analysis with Wikipedia requests for training and Stack Exchange requests for testing and using Linguistic classifiers are shown in table \ref{table:cross-domain-wiki-Ling}.

\begin{table*}[htbp]
\caption{Cross-domain analysis with Wikipedia requests for training and Stack Exchange requests for testing and using Linguistic classifiers }
\centering
\vspace{5pt}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Classifier} & \textbf{pre\_alpha} & \textbf{pre\_word} & \textbf{pre\_alpha\_with\_attribute\_selection} & \textbf{pre\_word\_with\_attribute\_selection} \\
\hline\hline
\textbf{Naive Bayes} & 64.4\% & 64.35\% & 65.55\% & 65.55\% \\ 
\hline
\textbf{Naive Bayes Multinomial} & 66.3\% & 66\% & 66.3\% & 66.5\% \\ 
\hline
\textbf{J48} & 61.45\% & 61.2\% & 61.05\% & 60.85\% \\ 
\hline
\textbf{Random Forest (10 trees)} & 63.95\% & 62.3\% & 65.1\% & 63.45\% \\ 
\hline
\textbf{Random Forest (100 trees)} & 62.85\% & 63.65\% & 64.65\% & 64.65\% \\ 
\hline
\textbf{iBK (k=1, using Euclidean Distance)} & 56.75\% & 56.75\% & 63.35\% & 62.5\% \\ 
\hline
\textbf{iBK (k=10, using Euclidean Distance)} & 51.85\% & 51.9\% & 60.4\% & 60.7\% \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 55.15\% & 55.65\% & 60.1\% & 59.6\% \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 51.35\% & 50.95\% & 58.05\% & 59.35\% \\ 
\hline
\textbf{SMO} & 64.9\% & 64.95\% & 65.8\% & 65.45\% \\ 
\hline
\hline
\end{tabular}
\label{table:cross-domain-wiki-Ling}
\end{table*}

The correctly classified instances (by \%) for Cross-domain analysis with Stack Exchange requests for training and Wikipedia requests for testing and using Bag of Words classifiers are shown in table \ref{table:cross-domain-stack-BOW}.

\begin{table*}[htbp]
\caption{Cross-domain analysis with Stack Exchange requests for training and Wikipedia requests for testing and using Bag of Words classifiers}
\centering
\vspace{5pt}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Classifier} & \textbf{pre\_alpha} & \textbf{pre\_word} & \textbf{pre\_alpha\_with\_attribute\_selection} & \textbf{pre\_word\_with\_attribute\_selection} \\
\hline\hline
\textbf{Naive Bayes} & 60.9375\% & 61.1213\% & 66.0386\% & 65.3493\% \\ 
\hline
\textbf{Naive Bayes Multinomial} & 68.9338\% & 68.75\% & 65.4412\% & 65.579\% \\ 
\hline
\textbf{J48} & 62.1783\% & 62.546\% & 64.0625\% & 64.7518\% \\ 
\hline
\textbf{Random Forest (10 trees)} & 66.9577\% & 64.568\% & 66.682\% & 67.6471\% \\ 
\hline
\textbf{Random Forest (100 trees)} & 70.5423\% & 68.9338\% & 68.1985\% & 68.4743\% \\ 
\hline
\textbf{iBK (k=1, using Euclidean Distance)} & 57.1691\% & 57.5827\% & 62.5919\% & 62.9596\% \\ 
\hline
\textbf{iBK (k=10, using Euclidean Distance)} & 53.3088\% & 52.8952\% & 66.5441\% & 66.4522\% \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 56.5257\% & 56.296\% & 61.2592\% & 61.6728\% \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 52.0221\% & 52.0221\% & 64.9816\% & 64.8897\% \\ 
\hline
\textbf{SMO} & 71.0938\% & 71.3235\% & 68.704\% & 68.75\% \\ 
\hline
\hline
\end{tabular}
\label{table:cross-domain-stack-BOW}
\end{table*}

The correctly classified instances (by \%) for Cross-domain analysis with Stack Exchange requests for training and Wikipedia requests for testing and using Linguistic classifiers are shown in table \ref{table:cross-domain-stack-Ling}.

\begin{table*}[htbp]
\caption{Cross-domain analysis with Stack Exchange requests for training and Wikipedia requests for testing and using Linguistic classifiers }
\centering
\vspace{5pt}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Classifier} & \textbf{pre\_alpha} & \textbf{pre\_word} & \textbf{pre\_alpha\_with\_attribute\_selection} & \textbf{pre\_word\_with\_attribute\_selection} \\
\hline\hline
\textbf{Naive Bayes} & 60.8915\% & 60.9835\% & 65.3952\% & 64.568\% \\ 
\hline
\textbf{Naive Bayes Multinomial} & 69.761\% & 69.6691\% & 68.0147\% & 68.2904\% \\ 
\hline
\textbf{J48} & 62.9136\% & 60.6618\% & 65.7169\% & 65.2114\% \\ 
\hline
\textbf{Random Forest (10 trees)} & 64.0625\% & 61.9945\% & 65.3952\% & 64.0625\% \\ 
\hline
\textbf{Random Forest (100 trees)} & 70.0368\% & 69.0257\% & 67.4632\% & 66.9577\% \\ 
\hline
\textbf{iBK (k=1, using Euclidean Distance)} & 58.1801\% & 57.8125\% & 57.5827\% & 58.1342\% \\ 
\hline
\textbf{iBK (k=10, using Euclidean Distance)} & 55.239\% & 53.3548\% & 63.1434\% & 62.8676\% \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 58.7776\% & 57.307\% & 57.5368\% & 57.9963\% \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 53.7224\% & 53.171\% & 64.1544\% & 64.0165\% \\ 
\hline
\textbf{SMO} & 70.9099\% & 71.6452\% & 69.761\% & 69.4393\% \\ 
\hline
\hline
\end{tabular}
\label{table:cross-domain-stack-Ling}
\end{table*}


\section{Related Work}

\section{Conclusions and Future Work}

\begin{thebibliography}{1}

\bibitem{Meier}
A. J. Meier. \textit{Defining Politeness: Universality in Appropriateness}

\bibitem{Jurafsky}
Cristian Danescu-Niculescu-Mizil, Moritz Sudhof, Dan Jurafsky, Jure Leskovec and Christopher Potts. \textit{A computational approach to politeness with application to social factors}.

\bibitem{Brown1}
BROWN, P. and LEVINSON, S. 1978. \textit{Universals in Language Usage: Politeness Phenomena}. In E. Goody (ed.), Questions and Politeness, 56-289. Cambridge University Press, Cambridge.

\bibitem{Brown2}
BROWN, P. and LEVINSON, S. 1987 \textit{Politeness: Some Universals in Language Usage}. Cambridge University Press, Cambridge.

\bibitem{Fraser}
Bruce Fraser. \textit{Perspectives on Politeness}

\bibitem{Mitchell}
Tom Mitchell. \textit{Machine Learning} McGraw-Hill Science/Engineering/Math; 1 edition (March 1, 1997)

\bibitem{Nigam}
Andrew McCallum and Kamal Nigam. \textit{A Comparison of Event Models for Naive Bayes Text Classification}

\bibitem{j48}
http://en.wikipedia.org/wiki/C4.5\_algorithm

\bibitem{breiman}
www.stat.berkeley.edu/~breiman/RandomForests/cc\_home.htm

\bibitem{smo}
http://en.wikipedia.org/wiki/Sequential\_minimal\_optimization

\end{thebibliography}



% that's all folks
\end{document}


