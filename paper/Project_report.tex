\documentclass[conference]{IEEEtran}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx} % Required to insert images
\usepackage{caption}
\newcommand{\Break}{\State \textbf{break} }
\algblockdefx[class]{class}{endclass}[1][]{\textbf{class} #1}{\textbf{End class}}
\algtext*{endclass}
\algblockdefx[method]{method}{endmethod}[1][]{\textbf{method} #1}{\textbf{End method}}
\algtext*{endmethod}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Comparative analysis of classifiers identifying politeness markings and application in web-logs}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Shagun Jhaver}
\IEEEauthorblockA{Dept. of Computer Science\\
The University of Texas at Dallas \\
Richardson, Texas 75080\\
Email: sxj124330@utdallas.edu}
\and
\IEEEauthorblockN{Latifur Khan}
\IEEEauthorblockA{Dept. of Computer Science\\
The University of Texas at Dallas \\
Richardson, Texas 75080\\
Email: lkhan@utdallas.edu}
}




% make the title area
\maketitle
\thispagestyle{plain}
\pagestyle{plain}


\begin{abstract}
%\boldmath
We develop a computational framework for identifying and characterizing politeness markings in text documents. We present a comparative study of the results of classifiers constructed using a variety of different algorithms, filters and features. We also use this framework to study the politeness levels in a diverse range of web-logs.

Index Terms - Politeness Theory, Classification, SMO
\end{abstract}
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle


\section{Introduction}
Politeness, deference and tact have a sociological significance altogether beyond the level of table manners and etiquette books (Goffman 1971:90). Politeness, introduced into linguistics more than forty years ago, has emerged as a vital and rapidly developing area of study. Brown and Levinson's (1978, 1987) classic treatment of linguistic politeness show that politeness strategies are a basis for social order. The concepts inherent to their model have been invoked in much subsequent literature which has focused on linguistic carriers of politeness (e.g., speech acts, syntactic constructions, lexical items, etc.), seeking to quantify them, to compare them across cultures and genders, and to identify universals \cite{Meier}.

Danescu-Niculescu-Mizil, Sudhof, Jurafsky, Leskovec and Potts \cite{Jurafsky} develop a computational framework for identifying and characterizing the linguistic aspects of politeness. Their investigation is guided by a new corpus of requests annotated for politeness, that they constructed and released. This corpus consists of a large collection of requests from two different sources - Wikipedia and Stack Exchange. Both of these are large online communities in which users frequently make requests of other members. 

In this paper, we use this richly labeled data for politeness to construct politeness classifiers using different supervised and unsupervised machine learning algorithms, and present a comparative analysis of the performance of these classifiers. We also study the improvement in classifiers' performance after they use a wide range of lexical, sentiment and dependency features operationalizing key components of politeness theory. 

We observe that some of our classifiers achieve near human-level accuracy across different test-sets, which demonstrates the consistent nature of politeness strategies, and we use these classifiers with new data for further analysis of the relation of politeness to social factors. We select the web-log (blog) entries from blogs focused at different interest groups, assign these entries a politeness score on a scale of 0 to 1 using our classifiers, and compare these scores. 


\section{Background}
The meaning of politeness and concomitant concepts, and the claims for universals have shown considerable divergence and lack of clarity as they have received increased attention since Brown and Lewinson's proposed framework \cite{Meier}, \cite{Brown1}, \cite{Brown2}. Scholars use a variety of approaches to an account of politeness: the social-norm view, the conversational-maxim view; the face-saving view; and the conversational-contract view \cite{Fraser}. While none of these views is considered adequate, the face-saving view by Brown and Levinson is seen as the most clearly articulated and is the most popular.

Brown and Levinson contend that linguistic politeness must be communicated, that it constitutes a message. They assert that the failure to communicate the intention to be polite may be taken as absence of the required polite attitude. They propose a framework to explain politeness in which their rational Model Person has `face', the individual's self-esteem. This face is a culturally elborated public self-image that every member of a society wants to claim for himself \cite{Fraser}. They characterize two types of face in terms of participant wants rather than social norms:

Negative Face:
``the want of every `competent adult member' that his action be unimpeded by others" 

Positive Face:
``the want of every member that his wants be desirable to at least some others"

The organizing principle for their politeness theory is the idea that ``some acts are intrinsically threatening to face and thus require softening ..." To this end, each group of language users develops politeness principles from which they derive certain linguistic strategies. It is by the use of these so-called politeness strategies that speakers succeed in communicating both their primary message(s) as well as their intention to be polite in doing so. And in doing so, they reduce the face loss that results from the interaction. 

The choice of a specific linguistic form is to be viewed as a specific realization of one of the politeness strategies in light of the speaker's assessment of the utterance context. Brown and Levinson outline four main types of politeness strategies: bald on-record, negative politeness, positive politeness, and off-record (indirect). The speaker must choose a linguistic means that will satisfy the strategic end. Since each strategy embraces a range of degrees of politeness, the speaker will be required to consider the specific linguistic forms used and their overall effect when used in conjunction with one another.

We try to identify such strategies and use them to construct our classifiers. A brief description of the classifiers we used is given below.

1) \textit{Naive Bayes}: Naive Bayes is a highly practical learning method whose performance is shown to be comparable to that of neural network and decision tree learning in some domains. It applies to the learning tasks where each instance $x$ is described by a conjunction of attribute values and where the target function $f(x)$ can take on any value from some finite set $V$. A set of training examples of the target function is provided, and a new instance is presented, described by the tuple of attribute values $\textless a_l, a_2,..., a_n\textgreater$. The learner is asked to predict the target value, or classification, for this new instance. The Bayesian approach to classifying the new instance is to assign the most probable target value, $V_{MAP}$, given the attribute values $\textless a_l, a_2,..., a_n\textgreater$ that describe the instance \cite{Mitchell}. 

The naive Bayes classifier is based on the simplifying assumption that the attribute values are conditionally independent given the target value. In other words, the assumption is that given the target value of the instance, the probability of observing the conjunction $a_l, a_2,..., a_n$ is just the product of the probabilities for the individual attributes.

2) \textit{Naive Bayes Multinomial}: In the multinomial model, a document is an ordered sequence of word events, drawn from the same vocabulary V. We assume that the lengths of documents are independent of class. We again make a similar naive Bayes assumption: that the probability of each word event in a document is independent of the word’s context and position in the document. Thus, each document $d_i$ is drawn from a multinomial distribution of words with as many independent trials as the length of $d_i$. This yields the familiar ``bag of words" representation for documents \cite{Nigam}. Whereas simple naive Bayes would model a document as the presence and absence of particular words, multinomial naive bayes explicitly models the word counts and adjusts the underlying calculations to deal with in. 

3) \textit{J48}: J48 is an open source Java implementation of the C4.5 algorithm in the weka data mining tool. C4.5 is an algorithm used to generate a decision tree developed by Ross Quinlan. C4.5 is an extension of Quinlan's earlier ID3 algorithm. The decision trees generated by C4.5 can be used for classification, and for this reason, C4.5 is often referred to as a statistical classifier. At each node of the tree, C4.5 chooses the attribute of the data that most effectively splits its set of samples into subsets enriched in one class or the other. The splitting criterion is the normalized information gain (difference in entropy). The attribute with the highest normalized information gain is chosen to make the decision. The C4.5 algorithm then recurses on the smaller sublists \cite{j48}.

4) \textit{Random Forest}: Random Forests grows many classification trees. To classify a new object from an input vector, put the input vector down each of the trees in the forest. Each tree gives a classification, and we say the tree "votes" for that class. The forest chooses the classification having the most votes (over all the trees in the forest).

Each tree is grown as follows:

\begin{enumerate}
\item If the number of cases in the training set is N, sample N cases at random - but with replacement, from the original data. This sample will be the training set for growing the tree.
\item If there are M input variables, a number m$<<$M is specified such that at each node, m variables are selected at random out of the M and the best split on these m is used to split the node. The value of m is held constant during the forest growing.
\item Each tree is grown to the largest extent possible. There is no pruning.
\end{enumerate}

In the original paper on random forests, it was shown that the forest error rate depends on two things:

\begin{enumerate}
\item The correlation between any two trees in the forest. Increasing the correlation increases the forest error rate.
\item The strength of each individual tree in the forest. A tree with a low error rate is a strong classifier. Increasing the strength of the individual trees decreases the forest error rate.
\end{enumerate}

Reducing m reduces both the correlation and the strength. Increasing it increases both. Somewhere in between is an ``optimal" range of m - usually quite wide. Using the oob error rate a value of m in the range can quickly be found. This is the only adjustable parameter to which random forests is somewhat sensitive \cite{breiman}.

5) \textit{IBk}: This is an implementation of the k-nearest neighbors algorithm. This basic instance-based algorithm assumes all instances correspond to points in the n-dimensional space. The nearest neighbors of an instance are defined in terms of the standard Euclidean or Manhattan distance. The value of the classification label for an input x returned by this algorithm is just the most common value of label among the k training examples nearest to x \cite{Mitchell}.

6) \textit{SMO}: Sequential minimal optimization (SMO) is an algorithm for solving the optimization problem which arises during the training of support vector machines. It was invented by John Platt in 1998 at Microsoft Research. SMO is widely used for training support vector machines and is implemented by the popular LIBSVM tool. SMO breaks the optimization problem in SVM into a series of smallest possible sub-problems, which are then solved analytically \cite{smo}.

\section{Experiments}
Our training data is from two different domains:
\begin{enumerate}
\item Wikipedia
\item Stack Exchange
\end{enumerate}

We experiment on two different types of classifiers: 
\begin{enumerate}
\item Bag of Words classifier (BOW)
\item Linguistically Informed classifer (Ling.)
\end{enumerate}

For Linguistically Informed classifer (Ling.), we use the features described in Table \ref{table:lingFeatures} \cite{Jurafsky}:
\begin{table*}[htbp]
\caption{Politeness Strategies used for features in Linguistically Informed Classifiers }
\centering
\vspace{5pt}
\begin{tabular}{|l|l|}
\hline
Strategy & Description \\
\hline\hline
Gratitude & Contains words like ``appreciate", ``thankful", ``grateful", ``recognize", ``indebted" \\
\hline
Deference & Contains words like ``Nice work", ``respect", ``polite" \\
\hline
Greeting & Use of words like ``Hey", ``Hi", ``Hello", ``take care", ``bye", ``Good morning", ``Dear", ``what's up", ``welcome" \\
\hline
Positive lexicon & Contains words in positive lexicon \\
\hline
Negative lexicon & Contains words in negative lexicon \\
\hline
Apologizing & Contains words like ``sorry", ``pardon", ``regret", ``apologize", ``ashamed", ``regretful", ``penitent" \\
\hline
Please & Contains ``please"\\
\hline
Please start & Starts with ``please"\\
\hline
Indirect (btw) & Contains phrases like ``by the way", ``btw" \\
\hline
Direct question  & Contains sentences beginning with ``wh" and ending with ``?" \\
\hline
Direct start & Contains sentences beginning with ``So", ``Well", etc \\
\hline
Counterfactual modal (Could/Would) & Contains sentences beginning with ``could", ``would", etc \\
\hline
Indicative modal (Can/Will) & Contains sentences beginning with ``can", ``will", etc \\
\hline
First Person Start & Contains sentences beginning with ``I", ``We", etc. \\
\hline
First Person plural & Use of words like ``We", etc. \\
\hline
First Person & Use of words like ``me", etc. \\
\hline
Second Person & Contains words like ``you", etc. \\
\hline
Second Person Start & Contains sentences beginning with ``you", ``your", etc. \\
\hline
Hedges & Contains phrases like ``I suggest" \\
\hline
Factuality & Contains phrases like ``In fact" \\
\hline
\hline
\end{tabular}
\label{table:lingFeatures}
\end{table*}

We use Weka (Waikato Environment for Knowledge Analysis), a popular suite of machine learning software written in Java, developed at the University of Waikato, New Zealand for all our experiments.

We run experiments of two types:
\begin{enumerate}
\item In-domain:
    We use 5-fold cross-validations for these experiments. The experiments are:
    \begin{itemize}
    \item Training on Wikipedia, Testing on Wikipedia
    \item Training on Stack-Exchange, Testing on Stack-Exchange
    \end{itemize}

\item Cross-domain:
    \begin{itemize}
    \item Training on Wikipedia, Testing on Stack-Exchange
    \item Training on Stack-Exchange, Testing on Wikipedia
    \end{itemize}
\end{enumerate}

The training and 5-fold cross-validation (In-domain) is done as follows:
\begin{enumerate}
\item Sort the training requests by their politeness scores.
\item Get top 25\% of requests, and label them as positive.
\item Get bottom 25\% of requests, and label them as negative.
\item Divide the data into 80\% for training and 20\% for testing.
\item Run classifier training procedure on training data.
\item Test the classifier on testing data.
\item Go back to Step 4 to repeat the procedure for different sets of training and testing data, and then take the average performance.
\end{enumerate}

For cross-domain experiments, we train the classifiers again using Steps 1-3 above. We use the alternate domain data for testing.

For each experiment type and classifier type, we have four sets of experiments:
\begin{enumerate}
\item Using String-to-word unsupervised filter with alphabetic tokenizer (pre\_alpha)
\item Using String-to-word unsupervised filter with alphabetic tokenizer followed by attribute selection (pre\_alpha\_with\_attribute\_selection)
\item Using String-to-word unsupervised filter with word tokenizer (pre\_word)
\item Using String-to-word unsupervised filter with word tokenizer followed by attribute selection (pre\_word\_with\_attribute\_selection)
\end{enumerate}


We use the following settings with String-to-word unsupervised filter:
\begin{itemize}
\item IDFTransform: True
\item TFTransform: True
\item attributeIndices: first-last
\item doNotOperateOnFirstClassBasis: False
\item invertSelection: False
\item lowerCaseTokens: False
\item minTermFrequency: 10
\item normalizeDocLength: No Normalization
\item outputWordCounts: True
\item periodicPruning: -1.0
\item stemmer: NullStemmer
\item stopwords: weka-3-6-10
\item useStoplist: False
\item wordsToKeep: 1000
\end{itemize}


For attribute selection, we use:
\begin{itemize}
\item evaluator: InfoGainAttributeEval and
\item search: Ranker with threshold 0.0
\end{itemize}

In each experiment set, we collect experiment results on these classifiers:
\begin{enumerate}
\item Naive Bayes
\item  Naive Bayes Multinomial
\item J48
\item Random Forest with:
    \begin{itemize}
    \item 10 trees
    \item 100 trees
    \end{itemize}
\item IBk (Instance-based k), the K-nearest neighbours classifier with:
    \begin{itemize}
    \item K=1 and using Euclidean distance
    \item K=10 and using Euclidean distance
    \item K=1 and using Manhattan distance
    \item K=10 and using Manhattan distance
    \end{itemize}
\item SMO (Support vector classifier)
\end{enumerate}

We observe that linguistically informed classifiers (Ling.) using String-to-word unsupervised filter with alphabetic tokenizer followed by attribute selection (pre\_alpha\_with\_attribute\_selection) generally give the best in-domain and cross-domain results. We use the classifiers to determine politeness in some blogs. We've used the blog entries from the blogs described in Table \ref{table:listBlogs}.
\begin{table*}[htbp]
\caption{Blogs used in testing}
\centering
\vspace{5pt}
\begin{tabular}{|l|l|l|}
\hline
Blog no. & url & Description \\
\hline\hline
blog 1 & http://blogs.wsj.com/peggynoonan/ & A Wall Street Journal Columnist \\
\hline
blog 2 & http://www.thefashionpolice.net/ & A blog about shopping and style \\
\hline
blog 3 & http://www.rogerebert.com/reviews/tyler-perrys-a-madea-christmas-2013 & A blog for Movie reviews \\
\hline
blog 4 & http://www.thedailybeast.com/ & A blog dedicated to breaking news and sharp commentary \\
\hline
blog 5 & http://www.blogcatalog.com/blogs/haters-be-hatin & A satirical humour blog \\
\hline
blog 6 & http://www.tmz.com/ & A celebrity news blog \\
\hline
blog 7 & http://www.samizdata.net/ & An individualistic perspective blog \\ 
\hline
blog 8 & http://waiterrant.net/ & A waiter's rant blog \\
\hline
blog 9 & http://www.hecklerspray.com/ & A gossip and reviews blog \\
\hline
blog 10 & http://wow.joystiq.com/ & A gaming blog \\
\hline
\hline
\end{tabular}
\label{table:listBlogs}
\end{table*}


\section{Experimental Results}

This section describes the results for the experiments. The percentage figures in the tables for In-domain and Cross-domain experiments denote the percentage of correctly classified instances.

\subsection{In-domain Experiments}
Four sets of experiments are done for in-domain analysis using a 5-fold cross-validation. 

The correctly classified instances (by \%) for In-domain analysis on Wikipedia requests using Bag of Words classifiers are shown in Table \ref{table:in-domain-wiki-BOW}.

\begin{table*}[htbp]
\caption{In-domain analysis on Wikipedia requests using Bag of Words classifiers }
\centering
\vspace{5pt}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Classifier} & \textbf{pre\_alpha} & \textbf{pre\_word} & \textbf{pre\_alpha\_with\_attribute\_selection} & \textbf{pre\_word\_with\_attribute\_selection} \\
\hline\hline
\textbf{Naive Bayes} & 74.5864\% & 74.4945\% & 77.2518\% & 77.068\% \\ 
\hline
\textbf{Naive Bayes Multinomial} & 78.9063\% & 79.6415\% & 80.5147\% & 80.1471\% \\ 
\hline
\textbf{J48} & 70.864\% & 71.2776\% & 73.7132\% & 74.3107\% \\ 
\hline
\textbf{Random Forest (10 trees)} & 74.5404\% & 73.6673\% & 76.7004\% & 76.6085\% \\ 
\hline
\textbf{Random Forest (100 trees)} & 80.7445\% & 80.193\% & 80.3309\% & 79.8254\% \\ 
\hline
\textbf{iBK (k=1, using Euclidean Distance)} & 64.8897\% & 64.1544\% & 71.2316\% & 70.6342\% \\ 
\hline
\textbf{iBK (k=10, using Euclidean Distance)} & 59.1912\% & 58.9154\% & 76.7463\% & 76.7923\% \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 63.2813\% & 63.1434\% & 71.2316\% & 69.1636\% \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 56.4338\% & 56.1581\% & 74.6783\% & 73.4835\% \\ 
\hline
\textbf{SMO} & 80.193\% & 79.8713\% & 82.307\% & 82.2151\% \\ 
\hline
\hline
\end{tabular}
\label{table:in-domain-wiki-BOW}
\end{table*}

The correctly classified instances (by \%) for In-domain analysis on Wikipedia requests using Linguistic classifiers are shown in Table \ref{table:in-domain-wiki-Ling}.

\begin{table*}[htbp]
\caption{In-domain analysis on Wikipedia requests using Linguistic classifiers }
\centering
\vspace{5pt}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Classifier} & \textbf{pre\_alpha} & \textbf{pre\_word} & \textbf{pre\_alpha\_with\_attribute\_selection} & \textbf{pre\_word\_with\_attribute\_selection} \\
\hline\hline
\textbf{Naive Bayes} & 74.4485\% & 74.7702\% & 77.0221\% & 76.3327\% \\ 
\hline
\textbf{Naive Bayes Multinomial} & 80.7904\% & 80.239\% & 80.4688\% & 80.3309\% \\ 
\hline
\textbf{J48} & 72.7022\% & 72.4265\% & 75\% & 73.3456\% \\ 
\hline
\textbf{Random Forest (10 trees)} & 72.932\% & 74.7702\% & 76.7004\% & 77.4357\% \\ 
\hline
\textbf{Random Forest (100 trees)} & 79.9173\% & 80.6066\% & 80.1011\% & 80.4228\% \\ 
\hline
\textbf{iBK (k=1, using Euclidean Distance)} & 64.6599\% & 64.8438\% & 71.4614\% & 71.829\% \\ 
\hline
\textbf{iBK (k=10, using Euclidean Distance)} & 60.2022\% & 59.6967\% & 76.5165\% & 76.7923\% \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 64.6599\% & 64.8897\% & 70.5423\% & 70.5423\% \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 59.4669\% & 59.5129\% & 74.6783\% & 74.9081\% \\ 
\hline
\textbf{SMO} & 81.3879\% & 80.3768\% & 82.2151\% & 81.0202\% \\ 
\hline
\hline
\end{tabular}
\label{table:in-domain-wiki-Ling}
\end{table*}

The correctly classified instances (by \%) for In-domain analysis on Stack Exchange requests using Bag of Words classifiers are shown in Table \ref{table:in-domain-stack-BOW}.

\begin{table*}[htbp]
\caption{In-domain analysis on Stack Exchange requests using Bag of Words classifiers }
\centering
\vspace{5pt}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Classifier} & \textbf{pre\_alpha} & \textbf{pre\_word} & \textbf{pre\_alpha\_with\_attribute\_selection} & \textbf{pre\_word\_with\_attribute\_selection} \\
\hline\hline
\textbf{Naive Bayes} & 68.4\% & 67.7\% & 71.8\% & 71.2\% \\ 
\hline
\textbf{Naive Bayes Multinomial} & 71.8\% & 71.55\% & 72.35\% & 71.4\% \\ 
\hline
\textbf{J48} & 67.15\% & 65.9\% & 69.05\% & 69.75\% \\ 
\hline
\textbf{Random Forest (10 trees)} & 69.5\% & 68.75\% & 70.15\% & 69.95\% \\ 
\hline
\textbf{Random Forest (100 trees)} & 73.6\% & 73.45\% & 72.35\% & 72.45\% \\ 
\hline
\textbf{iBK (k=1, using Euclidean Distance)} & 57.7\% & 58.85\% & 65.75\% & 65.6\% \\ 
\hline
\textbf{iBK (k=10, using Euclidean Distance)} & 53.2\% & 53.15\% & 66.95\% & 66.35\% \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 56.9\% & 56.85\% & 65.9\% & 63.55\% \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 51.35\% & 51.95\% & 64.6\% & 63.1\% \\ 
\hline
\textbf{SMO} & 74.55\% & 73.5\% & 74.8\% & 75.05\% \\ 
\hline
\hline
\end{tabular}
\label{table:in-domain-stack-BOW}
\end{table*}

The correctly classified instances (by \%) for In-domain analysis on Stack Exchange requests using Linguistic classifiers are shown in Table \ref{table:in-domain-stack-Ling}.

\begin{table*}[htbp]
\caption{In-domain analysis on Stack Exchange requests using Linguistic classifiers }
\centering
\vspace{5pt}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Classifier} & \textbf{pre\_alpha} & \textbf{pre\_word} & \textbf{pre\_alpha\_with\_attribute\_selection} & \textbf{pre\_word\_with\_attribute\_selection} \\
\hline\hline
\textbf{Naive Bayes} & 68.45\% & 68.55\% & 72.4\% & 72.4\% \\ 
\hline
\textbf{Naive Bayes Multinomial} & 72.85\% & 72.7\% & 74.6\% & 74.4\% \\ 
\hline
\textbf{J48} & 67.7\% & 67.25\% & 71.1\% & 69.65\% \\ 
\hline
\textbf{Random Forest (10 trees)} & 69.15\% & 67.35\% & 71.6\% & 70.25\% \\ 
\hline
\textbf{Random Forest (100 trees)} & 74.2\% & 74.15\% & 73.35\% & 72.75\% \\ 
\hline
\textbf{iBK (k=1, using Euclidean Distance)} & 58.4\% & 59.2\% & 64.9\% & 63.5\% \\ 
\hline
\textbf{iBK (k=10, using Euclidean Distance)} & 55.25\% & 57.65\% & 71.2\% & 71.15\% \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 58.3\% & 58.6\% & 63.8\% & 63.15\% \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 53.55\% & 54.45\% & 68.8\% & 67.85\% \\ 
\hline
\textbf{SMO} & 72.95\% & 73.95\% & 75\% & 75.95\% \\ 
\hline
\hline
\end{tabular}
\label{table:in-domain-stack-Ling}
\end{table*}

\subsection{Cross-domain Experiments}
Four sets of experiments are done for cross-domain analysis using a 5-fold cross-validation. 

The correctly classified instances (by \%) for Cross-domain analysis with Wikipedia requests for training and Stack Exchange requests for testing and using Bag of Words classifiers are shown in Table \ref{table:cross-domain-wiki-BOW}.

\begin{table*}[htbp]
\caption{Cross-domain analysis with Wikipedia requests for training and Stack Exchange requests for testing and using Bag of Words classifiers }
\centering
\vspace{5pt}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Classifier} & \textbf{pre\_alpha} & \textbf{pre\_word} & \textbf{pre\_alpha\_with\_attribute\_selection} & \textbf{pre\_word\_with\_attribute\_selection} \\
\hline\hline
\textbf{Naive Bayes} & 63.1\% & 62.7\% & 65.35\% & 64.85\% \\ 
\hline
\textbf{Naive Bayes Multinomial} & 66.45\% & 66\% & 66.55\% & 66.5\% \\ 
\hline
\textbf{J48} & 62.55\% & 61.6\% & 60.4\% & 61.1\% \\ 
\hline
\textbf{Random Forest (10 trees)} & 64.85\% & 64.95\% & 64.05\% & 64.35\% \\ 
\hline
\textbf{Random Forest (100 trees)} & 66.2\% & 66.25\% & 64.65\% & 64.65\% \\ 
\hline
\textbf{iBK (k=1, using Euclidean Distance)} & 55.05\% & 55\% & 62.6\% & 63.25\% \\ 
\hline
\textbf{iBK (k=10, using Euclidean Distance)} & 50.45\% & 50.25\% & 61.15\% & 61.35\% \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 54.7\% & 54.3\% & 61.05\% & 60.55\% \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 50.5\% & 50.35\% & 58.35\% & 58.75\% \\ 
\hline
\textbf{SMO} & 64.65\% & 65.55\% & 65.35\% & 64.4\% \\ 
\hline
\hline
\end{tabular}
\label{table:cross-domain-wiki-BOW}
\end{table*}

The correctly classified instances (by \%) for Cross-domain analysis with Wikipedia requests for training and Stack Exchange requests for testing and using Linguistic classifiers are shown in Table \ref{table:cross-domain-wiki-Ling}.

\begin{table*}[htbp]
\caption{Cross-domain analysis with Wikipedia requests for training and Stack Exchange requests for testing and using Linguistic classifiers }
\centering
\vspace{5pt}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Classifier} & \textbf{pre\_alpha} & \textbf{pre\_word} & \textbf{pre\_alpha\_with\_attribute\_selection} & \textbf{pre\_word\_with\_attribute\_selection} \\
\hline\hline
\textbf{Naive Bayes} & 64.4\% & 64.35\% & 65.55\% & 65.55\% \\ 
\hline
\textbf{Naive Bayes Multinomial} & 66.3\% & 66\% & 66.3\% & 66.5\% \\ 
\hline
\textbf{J48} & 61.45\% & 61.2\% & 61.05\% & 60.85\% \\ 
\hline
\textbf{Random Forest (10 trees)} & 63.95\% & 62.3\% & 65.1\% & 63.45\% \\ 
\hline
\textbf{Random Forest (100 trees)} & 62.85\% & 63.65\% & 64.65\% & 64.65\% \\ 
\hline
\textbf{iBK (k=1, using Euclidean Distance)} & 56.75\% & 56.75\% & 63.35\% & 62.5\% \\ 
\hline
\textbf{iBK (k=10, using Euclidean Distance)} & 51.85\% & 51.9\% & 60.4\% & 60.7\% \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 55.15\% & 55.65\% & 60.1\% & 59.6\% \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 51.35\% & 50.95\% & 58.05\% & 59.35\% \\ 
\hline
\textbf{SMO} & 64.9\% & 64.95\% & 65.8\% & 65.45\% \\ 
\hline
\hline
\end{tabular}
\label{table:cross-domain-wiki-Ling}
\end{table*}

The correctly classified instances (by \%) for Cross-domain analysis with Stack Exchange requests for training and Wikipedia requests for testing and using Bag of Words classifiers are shown in Table \ref{table:cross-domain-stack-BOW}.

\begin{table*}[htbp]
\caption{Cross-domain analysis with Stack Exchange requests for training and Wikipedia requests for testing and using Bag of Words classifiers}
\centering
\vspace{5pt}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Classifier} & \textbf{pre\_alpha} & \textbf{pre\_word} & \textbf{pre\_alpha\_with\_attribute\_selection} & \textbf{pre\_word\_with\_attribute\_selection} \\
\hline\hline
\textbf{Naive Bayes} & 60.9375\% & 61.1213\% & 66.0386\% & 65.3493\% \\ 
\hline
\textbf{Naive Bayes Multinomial} & 68.9338\% & 68.75\% & 65.4412\% & 65.579\% \\ 
\hline
\textbf{J48} & 62.1783\% & 62.546\% & 64.0625\% & 64.7518\% \\ 
\hline
\textbf{Random Forest (10 trees)} & 66.9577\% & 64.568\% & 66.682\% & 67.6471\% \\ 
\hline
\textbf{Random Forest (100 trees)} & 70.5423\% & 68.9338\% & 68.1985\% & 68.4743\% \\ 
\hline
\textbf{iBK (k=1, using Euclidean Distance)} & 57.1691\% & 57.5827\% & 62.5919\% & 62.9596\% \\ 
\hline
\textbf{iBK (k=10, using Euclidean Distance)} & 53.3088\% & 52.8952\% & 66.5441\% & 66.4522\% \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 56.5257\% & 56.296\% & 61.2592\% & 61.6728\% \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 52.0221\% & 52.0221\% & 64.9816\% & 64.8897\% \\ 
\hline
\textbf{SMO} & 71.0938\% & 71.3235\% & 68.704\% & 68.75\% \\ 
\hline
\hline
\end{tabular}
\label{table:cross-domain-stack-BOW}
\end{table*}

The correctly classified instances (by \%) for Cross-domain analysis with Stack Exchange requests for training and Wikipedia requests for testing and using Linguistic classifiers are shown in Table \ref{table:cross-domain-stack-Ling}.

\begin{table*}[htbp]
\caption{Cross-domain analysis with Stack Exchange requests for training and Wikipedia requests for testing and using Linguistic classifiers }
\centering
\vspace{5pt}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Classifier} & \textbf{pre\_alpha} & \textbf{pre\_word} & \textbf{pre\_alpha\_with\_attribute\_selection} & \textbf{pre\_word\_with\_attribute\_selection} \\
\hline\hline
\textbf{Naive Bayes} & 60.8915\% & 60.9835\% & 65.3952\% & 64.568\% \\ 
\hline
\textbf{Naive Bayes Multinomial} & 69.761\% & 69.6691\% & 68.0147\% & 68.2904\% \\ 
\hline
\textbf{J48} & 62.9136\% & 60.6618\% & 65.7169\% & 65.2114\% \\ 
\hline
\textbf{Random Forest (10 trees)} & 64.0625\% & 61.9945\% & 65.3952\% & 64.0625\% \\ 
\hline
\textbf{Random Forest (100 trees)} & 70.0368\% & 69.0257\% & 67.4632\% & 66.9577\% \\ 
\hline
\textbf{iBK (k=1, using Euclidean Distance)} & 58.1801\% & 57.8125\% & 57.5827\% & 58.1342\% \\ 
\hline
\textbf{iBK (k=10, using Euclidean Distance)} & 55.239\% & 53.3548\% & 63.1434\% & 62.8676\% \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 58.7776\% & 57.307\% & 57.5368\% & 57.9963\% \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 53.7224\% & 53.171\% & 64.1544\% & 64.0165\% \\ 
\hline
\textbf{SMO} & 70.9099\% & 71.6452\% & 69.761\% & 69.4393\% \\ 
\hline
\hline
\end{tabular}
\label{table:cross-domain-stack-Ling}
\end{table*}

\subsection{Experiments on web logs}

We now use some of the best classifiers we observed in the previous experiments to determine the politeness for blog entries of some popular blogs. The source for these blogs are discussed in the `Experiments' section. For each experiment, we show the probability that the classifier assigns to each blog entry being `polite' and being `impolite'. These probabilities are on a scale of 0 to 1.

The classification results for blog 1 - blog 5 using Wikipedia requests for training are shown in Table \ref{table:blog-wiki}. For this, we use linguistic classifiers (Ling.) applying String-to-word unsupervised filter with alphabetic tokenizer followed by attribute selection (pre\_alpha\_with\_attribute\_selection). 

\begin{table*}[htbp]
\caption{Classification results using Wikipedia requests for training for blog 1 - blog 5}
\centering
\vspace{5pt}
\begin{tabular}{|l|c|c|l|l|c|c|l|l|c|c|}
\hline
\textbf{Classifier} & \multicolumn{2}{|c|}{\textbf{Blog 1}} & \multicolumn{2}{|c|}{\textbf{Blog 2}} & \multicolumn{2}{|c|}{\textbf{Blog 3}} & \multicolumn{2}{|c|}{\textbf{Blog 4}} & \multicolumn{2}{|c|}{\textbf{Blog 5}} \\
\hline
& polite & impolite & polite & impolite & polite & impolite & polite & impolite & polite & impolite \\
\hline\hline
\textbf{Naive Bayes} & 0.0 & 1.0 & 0.0 & 1.0 & 1.0 & 0.0 & 1.0 & 0.0 & 0.0 & 1.0 \\ 
\hline
\textbf{Naive Bayes Multinomial} & 1.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 \\ 
\hline
\textbf{J48} & 0.0 & 1.0 & 0.25 & 0.75 & 0.034 & 0.966 & 0.25 & 0.75 & 0.034 & 0.966 \\ 
\hline
\textbf{Random Forest (10 trees)} & 0.3 & 0.7 & 0.3 & 0.7 & 0.4 & 0.6 & 0.4 & 0.6 & 0.3 & 0.7 \\ 
\hline
\textbf{Random Forest (100 trees)} & 0.33 & 0.67 & 0.46 & 0.54 & 0.47 & 0.53 & 0.54 & 0.46 & 0.42 & 0.58 \\ 
\hline
\textbf{iBK (k=1, using Euclidean Distance)} & 0.0 & 1.0 & 0.0 & 1.0 & 1.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 \\ 
\hline
\textbf{iBK (k=10, using Euclidean Distance)} & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.7 & 0.3 & 0.6 & 0.4 \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 0.0 & 1.0 & 1.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 \\ 
\hline
\textbf{iBK (k=10, using Manhattan Distance)} & 0.4 & 0.6 & 0.5 & 0.5 & 0.5 & 0.5 & 0.7 & 0.3 & 0.6 & 0.4 \\ 
\hline
\textbf{SMO} & 1.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 \\ 
\hline
\hline
\end{tabular}
\label{table:blog-wiki}
\end{table*}

The classification results for blog 1 - blog 5 using Stack Exchange requests for training are shown in Table \ref{table:blog-stack}. For this, we again use linguistic classifiers (Ling.) applying String-to-word unsupervised filter with alphabetic tokenizer followed by attribute selection (pre\_alpha\_with\_attribute\_selection). 

\begin{table*}[htbp]
\caption{Classification results using Stack Exchange requests for training for blog 1 - blog 5}
\centering
\vspace{5pt}
\begin{tabular}{|l|c|c|l|l|c|c|l|l|c|c|}
\hline
\textbf{Classifier} & \multicolumn{2}{|c|}{\textbf{Blog 1}} & \multicolumn{2}{|c|}{\textbf{Blog 2}} & \multicolumn{2}{|c|}{\textbf{Blog 3}} & \multicolumn{2}{|c|}{\textbf{Blog 4}} & \multicolumn{2}{|c|}{\textbf{Blog 5}} \\
\hline
& polite & impolite & polite & impolite & polite & impolite & polite & impolite & polite & impolite \\
\hline\hline
\textbf{Naive Bayes} & 1.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 & 0.0 & 1.0 \\ 
\hline
\textbf{Naive Bayes Multinomial} & 1.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 \\ 
\hline
\textbf{J48} & 0.0 & 1.0 & 0.0 & 1.0 & 0.049 & 0.951 & 0.0 & 1.0 & 0.049 & 0.951 \\ 
\hline
\textbf{Random Forest (10 trees)} & 0.8 & 0.2 & 0.8 & 0.2 & 0.7 & 0.3 & 0.6 & 0.4 & 0.8 & 0.2 \\ 
\hline
\textbf{Random Forest (100 trees)} & 0.7 & 0.3 & 0.67 & 0.33 & 0.49 & 0.51 & 0.68 & 0.32 & 0.45 & 0.55 \\ 
\hline
\textbf{iBK (k=1, using Euclidean Distance)} & 0.0 & 1.0 & 1.0 & 0.0 & 1.0 & 0.0 & 0.0 & 1.0 & 1.0 & 0.0 \\ 
\hline
\textbf{iBK (k=10, using Euclidean Distance)} & 0.7 & 0.3 & 0.8 & 0.2 & 0.6 & 0.4 & 0.4 & 0.6 & 0.9 & 0.1 \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 1.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 & 0.0 & 1.0 & 0.0 & 1.0 \\ 
\hline
\textbf{iBK (k=10, using Manhattan Distance)} & 0.7 & 0.3 & 0.8 & 0.2 & 0.6 & 0.4 & 0.6 & 0.4 & 0.4 & 0.6 \\ 
\hline
\textbf{SMO} & 1.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 \\ 
\hline
\hline
\end{tabular}
\label{table:blog-stack}
\end{table*}



The classification results for blog 6 - blog 10 using Wikipedia requests for training are shown in Table \ref{table:kblog-wiki}. For this, we use linguistic classifiers (Ling.) applying String-to-word unsupervised filter with alphabetic tokenizer followed by attribute selection (pre\_alpha\_with\_attribute\_selection). 

\begin{table*}[htbp]
\caption{Classification results using Wikipedia requests for training for blog 6 - blog 10}
\centering
\vspace{5pt}
\begin{tabular}{|l|c|c|l|l|c|c|l|l|c|c|}
\hline
\textbf{Classifier} & \multicolumn{2}{|c|}{\textbf{Blog 6}} & \multicolumn{2}{|c|}{\textbf{Blog 7}} & \multicolumn{2}{|c|}{\textbf{Blog 8}} & \multicolumn{2}{|c|}{\textbf{Blog 9}} & \multicolumn{2}{|c|}{\textbf{Blog 10}} \\
\hline
& polite & impolite & polite & impolite & polite & impolite & polite & impolite & polite & impolite \\
\hline\hline
\textbf{Naive Bayes} & 0.001 & 0.999 & 0.0 & 1.0 & 1.0 & 0.0 & 0.0 & 1.0 & 1.0 & 0.0 \\ 
\hline
\textbf{Naive Bayes Multinomial} & 1.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 \\ 
\hline
\textbf{J48} & 0.0 & 1.0 & 0.667 & 0.333 & 0.961 & 0.039 & 0.081 & 0.919 & 0.667 & 0.333 \\ 
\hline
\textbf{Random Forest (10 trees)} & 0.5 & 0.5 & 0.5 & 0.5 & 0.6 & 0.4 & 0.4 & 0.6 & 0.5 & 0.5 \\ 
\hline
\textbf{Random Forest (100 trees)} & 0.4 & 0.6 & 0.35 & 0.65 & 0.53 & 0.47 & 0.37 & 0.63 & 0.56 & 0.44 \\ 
\hline
\textbf{iBK (k=1, using Euclidean Distance)} & 1.0 & 0.0 & 1.0 & 0.0 & 0.0 & 1.0 & 0.0 & 1.0 & 1.0 & 0.0 \\ 
\hline
\textbf{iBK (k=10, using Euclidean Distance)} & 0.3 & 0.7 & 0.4 & 0.6 & 0.5 & 0.5 & 0.0 & 1.0 & 0.6 & 0.4 \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 1.0 & 0.0 & 1.0 & 0.0 & 0.0 & 1.0 & 0.0 & 1.0 & 1.0 & 0.0 \\ 
\hline
\textbf{iBK (k=10, using Manhattan Distance)} & 0.3 & 0.7 & 0.5 & 0.5 & 0.4 & 0.6 & 0.4 & 0.6 & 0.7 & 0.3 \\ 
\hline
\textbf{SMO} & 1.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 \\ 
\hline
\hline
\end{tabular}
\label{table:kblog-wiki}
\end{table*}


The classification results for blog 6 - blog 10 using Stack Exchange requests for training are shown in Table \ref{table:kblog-stack}. For this, we again use linguistic classifiers (Ling.) applying String-to-word unsupervised filter with alphabetic tokenizer followed by attribute selection (pre\_alpha\_with\_attribute\_selection). 

\begin{table*}[htbp]
\caption{Classification results using Stack Exchange requests for training for blog 6 - blog 10}
\centering
\vspace{5pt}
\begin{tabular}{|l|c|c|l|l|c|c|l|l|c|c|}
\hline
\textbf{Classifier} & \multicolumn{2}{|c|}{\textbf{Blog 6}} & \multicolumn{2}{|c|}{\textbf{Blog 7}} & \multicolumn{2}{|c|}{\textbf{Blog 8}} & \multicolumn{2}{|c|}{\textbf{Blog 9}} & \multicolumn{2}{|c|}{\textbf{Blog 10}} \\
\hline
& polite & impolite & polite & impolite & polite & impolite & polite & impolite & polite & impolite \\
\hline\hline
\textbf{Naive Bayes} & 1.0 & 0.0 & 0.0 & 1.0 & 1.0 & 0.0 & 0.0 & 1.0 & 1.0 & 0.0 \\ 
\hline
\textbf{Naive Bayes Multinomial} & 1.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 \\ 
\hline
\textbf{J48} & 0.958 & 0.042 & 0.0 & 1.0 & 0.0 & 1.0 & 0.75 & 0.25 & 0.0 & 1.0 \\ 
\hline
\textbf{Random Forest (10 trees)} & 0.7 & 0.3 & 0.7 & 0.3 & 0.8 & 0.2 & 0.5 & 0.5 & 0.5 & 0.5 \\ 
\hline
\textbf{Random Forest (100 trees)} & 0.73 & 0.27 & 0.65 & 0.35 & 0.73 & 0.27 & 0.47 & 0.53 & 0.71 & 0.29 \\ 
\hline
\textbf{iBK (k=1, using Euclidean Distance)} & 1.0 & 0.0 & 0.0 & 1.0 & 1.0 & 0.0 & 0.0 & 1.0 & 1.0 & 0.0 \\ 
\hline
\textbf{iBK (k=10, using Euclidean Distance)} & 0.3 & 0.7 & 0.2 & 0.8 & 0.5 & 0.5 & 0.273 & 0.727 & 0.5 & 0.5 \\ 
\hline
\textbf{iBK (k=1, using Manhattan Distance)} & 0.0 & 1.0 & 0.0 & 1.0 & 1.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 \\ 
\hline
\textbf{iBK (k=10, using Manhattan Distance)} & 0.5 & 0.5 & 0.3 & 0.7 & 0.6 & 0.4 & 0.273 & 0.727 & 0.4 & 0.6 \\ 
\hline
\textbf{SMO} & 1.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 \\ 
\hline
\hline
\end{tabular}
\label{table:kblog-stack}
\end{table*}

\section{Related Work}
Politeness is a source of pragmatic enrichment, social meaning, and cultural variation \cite{Jurafsky}. The social-norm view of politeness reflects the historical understanding of politeness generally embraced by the public within the English-speaking world. It assumes that each society has a particular set of social norms consisting of more or less explicit rules that prescribe a certain behavior, a state of affairs, or a way of thinking in a context. A positive evaluation (politeness) arises when an action is in congruence with the norm, a negative evaluation (impoliteness = rudeness) when action is to the contrary \cite{Fraser}.

Manuals of etiquette contain aphorisms that reveal quickly this underlying assumption. The 1872 version of \textit{Ladies' Book of Etiquette and Manual of Politeness} (J. S. Locke, Boston, cited in Kasher (1986)) offers a variety of rules intended to govern polite discourse.

The conversational-maxim perspective on politeness relies principally on the work of Grice (1967, published 1975) in his now-classic paper `Logic and conversation'. Grice argued that conversationalists are rational individuals who are, all other things being equal, primarily interested in the efficient conveying of messages \cite{Fraser}.

By far, the most popular view of politeness is the face-saving view by Brown and Levinson. The aspects of their theory have have been explored from game-theoretic perspectives (Van Rooy, 2003) and implemented in language generation systems for interactive narratives (Walker et al., 1997), cooking instructions, (Gupta et al., 2007), translation (Faruqui and Pado, 2012), spoken dialog (Wang et al., 2012), and subjectivity analysis (Abdul-Mageed and Diab, 2012), among others.

In recent years, politeness has been studied in web environments. Politeness variations across different social groups (Burke and Kraut, 2008a) and different media types (Herring, 1994; Brennan and Ohaeri, 1999; Duthler, 2006) have been researched. Danescu-Niculescu-Mizil, Sudhof, Jurafsky, Leskovec and Potts \cite{Jurafsky} pursue similar goals and construct annotated data orders of magnitude larger for a more reliable study of politeness strategies. The present paper uses this annotated data for performing classifications.

Some researchers have also focused on domain-specific textual cues to study how language relates to power and status in the context of social networking (Scholand et al., 2010) and workplace discourse (Bramsen et al., ; Diehl et al., 2007; Peterson et al., 2011; Prabhakaran et al., 2012; Gilbert, 2012; McCallum et al., 2007).

\section{Conclusions and Future Work}
We train classifiers employing varying machine learning algorithms and using unsupervised and supervised filters and perform experiments in in-domain and cross-domain environments. We use linguistic features and expect the performance of classifiers to improve. We observe that in general, SMO classifiers tend to give the best performance for text classifications. The performance of SMO algorithms improve when we apply String-to-word unsupervised filter with alphabetic tokenizer followed by attribute selection. 

For in-domain experiments, classifiers that don't use attribute selection, using linguistic features for training improves the performance by 2-3\%. This improvement reduces to 0-1\% when attribute selection is also used while training the classifiers. In almost every case, using linguistic features registers an improvement in the performance.

These classifers perform well in cross-domain settings too. When training on Wikipedia and tested on Stack Exchange, the best performance is 65.8\%. When training on Stack Exchange and tested on Wikipedia, the best performance is 71.64\%. SMO classifiers using linguistic features show these best results.

We use the better performing classifiers to determine the politeness for blog entries of some popular blogs. We observe that a majority of classifiers classify Blogs 1, 2, 3, 4, 6, 7, 8 and 10 as polite, and Blogs 5 and 9 as impolite.

In future, we'd employ AdaBoosting techniques and infer if the performance improves for these experiments. We also plan to use domain adaptation techniques and use these classifiers to determine politeness in domains other than web-logs.

\begin{thebibliography}{1}

\bibitem{Meier}
A. J. Meier. \textit{Defining Politeness: Universality in Appropriateness}

\bibitem{Jurafsky}
Cristian Danescu-Niculescu-Mizil, Moritz Sudhof, Dan Jurafsky, Jure Leskovec and Christopher Potts. \textit{A computational approach to politeness with application to social factors}.

\bibitem{Brown1}
BROWN, P. and LEVINSON, S. 1978. \textit{Universals in Language Usage: Politeness Phenomena}. In E. Goody (ed.), Questions and Politeness, 56-289. Cambridge University Press, Cambridge.

\bibitem{Brown2}
BROWN, P. and LEVINSON, S. 1987 \textit{Politeness: Some Universals in Language Usage}. Cambridge University Press, Cambridge.

\bibitem{Fraser}
Bruce Fraser. \textit{Perspectives on Politeness}

\bibitem{Mitchell}
Tom Mitchell. \textit{Machine Learning} McGraw-Hill Science/Engineering/Math; 1 edition (March 1, 1997)

\bibitem{Nigam}
Andrew McCallum and Kamal Nigam. \textit{A Comparison of Event Models for Naive Bayes Text Classification}

\bibitem{j48}
http://en.wikipedia.org/wiki/C4.5\_algorithm

\bibitem{breiman}
www.stat.berkeley.edu/~breiman/RandomForests/cc\_home.htm

\bibitem{smo}
http://en.wikipedia.org/wiki/Sequential\_minimal\_optimization

\bibitem{}
Vinodkumar Prabhakaran, Owen Rambow, and Mona Diab. 2012. \textit{Predicting Overt Display of Power in Written Dialogs}. In Proceedings of NAACL-HLT, pages 518–522.

\bibitem{}
Kelly Peterson, Matt Hohensee, and Fei Xia. 2011. \textit{Email formality in the workplace: A case study on the enron corpus}. In Proceedings of the ACL Workshop on Language in Social Media, pages 86–95

\bibitem{}
Minqing Hu and Bing Liu. \textit{Mining and Summarizing Customer Reviews.} Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2004), Aug 22-25, 2004, Seattle, Washington, USA

\bibitem{}
Bing Liu, Minqing Hu and Junsheng Cheng. \textit{Opinion Observer: Analyzing and Comparing Opinions on the Web.} Proceedings of the 14th International World Wide Web conference (WWW-2005), May 10-14, 2005, Chiba, Japan.

\end{thebibliography}



% that's all folks
\end{document}


